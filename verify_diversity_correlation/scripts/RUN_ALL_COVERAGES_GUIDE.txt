USAGE GUIDE: run_all_coverages.sh
==================================

Purpose:
--------
Runs hash-diversity correlation analysis for ALL samples across ALL coverage thresholds
in a single automated workflow.

Coverage Values:
----------------
The script tests 8 coverage thresholds:
  - 1.0      (full coverage)
  - 0.5      (1/2)
  - 0.25     (1/4)
  - 0.125    (1/8)
  - 0.0625   (1/16)
  - 0.03125  (1/32)
  - 0.015625 (1/64)
  - 0.0      (no coverage requirement)

Basic Usage:
------------
Simply run (no confirmation needed - starts immediately):
    bash run_all_coverages.sh

Or in the background:
    nohup bash run_all_coverages.sh &

Or with screen/tmux (recommended for long runs):
    screen -S hash_analysis
    bash run_all_coverages.sh
    # Ctrl+A, D to detach

What It Does:
-------------
1. Shows configuration and estimated runtime
2. Starts analysis immediately (no confirmation prompt)
3. Runs analysis for each coverage value sequentially
4. Creates separate output directory for each coverage:
   /scratch/dmk333_new/Logan/Logan_Analyses/verify_diversity_correlation/data/hash_diversity_results_full_cov_${coverage}/

5. Tracks successes and failures
6. Generates comprehensive log file
7. Provides final summary

Features:
---------
✓ Non-interactive - starts immediately, no prompts
✓ Automatic logging to run_all_coverages.log
✓ Error handling - continues on failure
✓ Progress tracking with timestamps
✓ Success/failure summary at end
✓ Uses all available samples (no subsampling)
✓ Uses 200 cores (leaves 56 free on your 256-core server)

Estimated Runtime:
------------------
Per coverage:  4-8 hours
Total:         32-64 hours (1.3-2.7 days)

This depends on:
- Number of WGS samples in database (~500k?)
- Database I/O speed
- System load

Configuration:
--------------
Edit these variables in the script if needed:

N_JOBS=200
  - Number of parallel workers
  - Default: 200 (leaves cores for system)
  - Max recommended: 256 (uses all cores)

BASE_OUTPUT_DIR="/scratch/dmk333_new/Logan/Logan_Analyses/verify_diversity_correlation/data"
  - Where results are saved
  - Modify if you want different location

Output Structure:
-----------------
For each coverage value, creates:

hash_diversity_results_full_cov_${coverage}/
├── plots/
│   ├── hash_diversity_correlation.png    # Main result
│   ├── hash_diversity_hexbin.png
│   ├── distributions.png
│   └── residuals.png
├── data/
│   ├── hash_diversity_data.csv           # Full dataset
│   └── selected_samples.csv
└── reports/
    ├── analysis_report.txt               # Interpretation
    └── statistics_summary.csv

Plus:
  run_all_coverages.log                   # Complete log of all runs

Monitoring Progress:
--------------------
While running, you can:

# Watch the log file
tail -f /scratch/dmk333_new/Logan/Logan_Analyses/verify_diversity_correlation/data/run_all_coverages.log

# Check CPU usage
top -u dmk333

# Check which coverage is running
ps aux | grep hash_diversity_correlation

# See output directories created so far
ls -lht /scratch/dmk333_new/Logan/Logan_Analyses/verify_diversity_correlation/data/

Stopping the Run:
-----------------
If you need to stop:
1. Ctrl+C if in foreground
2. kill <PID> if in background

The script will complete the current coverage before stopping.
Already completed coverage values will be preserved.

Resuming After Failure:
-----------------------
If a coverage fails or you stop the script:
1. Check which coverage values completed successfully
2. Edit the COVERAGES array in the script to remove completed ones
3. Re-run the script

Example - if 1.0, 0.5, and 0.25 completed:
    COVERAGES=(0.125 0.0625 0.03125 0.015625 0.0)

Interpreting Results:
---------------------
After completion, compare results across coverage thresholds:

1. Check correlation strength (Pearson r) in each report
2. Look for patterns - does r increase/decrease with coverage?
3. Examine sample counts - fewer samples at higher coverage thresholds
4. Compare main plots visually

Questions to answer:
- Is correlation robust across coverage thresholds?
- What's the optimal coverage for your analysis?
- Do you see consistent biological signal?

Troubleshooting:
----------------
Problem: "Database locked" errors
Solution: Reduce N_JOBS to decrease concurrent database connections

Problem: Very slow progress
Solution: Check database I/O with iostat, may be I/O bound

Problem: Out of memory
Solution: Reduce N_JOBS (each worker uses ~2-4GB)

Problem: Script stops unexpectedly
Solution: Check run_all_coverages.log for error messages

Example Log Output:
-------------------
=======================================================================
HASH-DIVERSITY CORRELATION ANALYSIS - ALL COVERAGE THRESHOLDS
=======================================================================
Starting: Mon Dec  9 15:30:00 EST 2024
Coverage values: 1.0 0.5 0.25 0.125 0.0625 0.03125 0.015625 0.0
...

-----------------------------------------------------------------------
Processing coverage = 1.0
Output directory: .../hash_diversity_results_full_cov_1.0
Started: Mon Dec  9 15:30:05 EST 2024
-----------------------------------------------------------------------
[... analysis output ...]
✓ SUCCESS: Completed coverage = 1.0 at Mon Dec  9 21:45:23 EST 2024
-----------------------------------------------------------------------

[... continues for each coverage ...]

=======================================================================
ALL ANALYSES COMPLETE
=======================================================================
Summary:
  Successful: 8/8
  Failed: 0/8

✓ Successful coverage values:
    1.0 → .../hash_diversity_results_full_cov_1.0/
    0.5 → .../hash_diversity_results_full_cov_0.5/
    ...

Tips:
-----
1. Run overnight/over weekend since it takes 1-3 days
2. Use screen or tmux to keep running if SSH disconnects
3. Check the first coverage completes successfully before leaving
4. Monitor log file periodically
5. Have ~50GB free space for all outputs combined

Post-Analysis:
--------------
After all coverage values complete, you can:

1. Create comparative plots across coverage thresholds
2. Identify optimal coverage for your downstream analysis
3. Generate supplementary figures for publication
4. Use the CSV files for meta-analysis

The hash_diversity_sensitivity.py script can also help with
comparative analysis if you want to process it further.
