#!/usr/bin/env python3
"""
Functional Hash-Diversity Correlation Analysis
================================================
Tests the hypothesis that distinct k-mer hashes per basepair correlates with functional diversity
(KEGG KO richness and abundance-aware metrics) in WGS metagenomic samples.

This script analyzes FracMinHash sketches (k=33 amino acid / scale=1000) generated by fmh-funprofiler.
The hash counts represent approximately 1/1000 of the 11-mers (amino acid) present in each sample.

This script:
1. Queries WGS metagenomic samples from metadata database
2. Extracts hash counts from functional_profile_data.gather_data
3. Extracts KO abundance data from functional_profile.profiles
4. Calculates multiple diversity metrics:
   - Observed Richness (number of KOs)
   - Shannon Index (H')
   - Simpson's Index (D) and Gini-Simpson (1-D)
   - Hill Number of Order 2 (effective number of species)
   - Berger-Parker Index (dominance)
   - Pielou's Evenness (J')
5. Calculates normalized metrics (per megabase)
6. Performs correlation analysis for each diversity metric
7. Generates publication-quality visualizations

Usage:
    python functional_hash_diversity_correlation.py --output-dir results --n-jobs 128
"""

from __future__ import annotations

import argparse
import sys
import time
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import warnings
from datetime import datetime

import duckdb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr, spearmanr, kendalltau
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def log_step(step_name: str, start: bool = True):
    """Log the start or end of a processing step with timestamp."""
    if start:
        logger.info(f"{'='*60}")
        logger.info(f"STARTING: {step_name}")
        logger.info(f"{'='*60}")
    else:
        logger.info(f"COMPLETED: {step_name}")
        logger.info(f"{'-'*60}")

# Import optional correlation libraries
try:
    import dcor
    HAS_DCOR = True
except ImportError:
    HAS_DCOR = False
    # Note: dcor not installed - distance correlation will be skipped

try:
    from minepy import MINE
    HAS_MINE = True
except ImportError:
    HAS_MINE = False
    # Note: minepy not installed - MIC will be skipped

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set publication-quality style
sns.set_style("whitegrid")
sns.set_context("paper", font_scale=1.5)
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']

# Database paths
YACHT_DB = "/scratch/shared_data_new/Logan_yacht_data/processed_data/database_all.db"
METADATA_DB = "/scratch/shared_data_new/Logan_yacht_data/metadata/aws_sra_metadata/metadata_geo_joined_5M.duckdb"

# Diversity metrics to calculate
DIVERSITY_METRICS = [
    'observed_richness',
    'shannon_index',
    'simpson_index',
    'gini_simpson',
    'hill_2',
    'berger_parker',
    'pielou_evenness'
]


@dataclass
class Config:
    """Configuration for the analysis."""
    output_dir: Path
    n_samples: Optional[int] = None  # None = all samples
    n_jobs: int = 64  # Number of parallel workers
    random_seed: int = 42
    min_mbases: float = 100.0
    normalization_factor: float = 1_000_000.0  # Per million bases
    figure_size: Tuple[float, float] = (10, 8)
    dpi: int = 300
    max_samples_for_expensive_corr: int = 50000  # Subsample for dcor/MIC if larger
    
    def __post_init__(self):
        self.output_dir = Path(self.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "plots").mkdir(exist_ok=True)
        (self.output_dir / "data").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)


def compute_expensive_correlations(args: Tuple) -> Dict:
    """
    Compute expensive correlation metrics (distance correlation, MIC) for a single metric.
    Designed to be run in parallel.
    
    Args:
        args: Tuple of (metric_col, metric_name, x_array, y_array, max_samples)
        
    Returns:
        Dictionary with metric results
    """
    metric_col, metric_name, x, y, max_samples = args
    
    n_samples = len(x)
    results = {
        'metric_col': metric_col,
        'metric_name': metric_name,
        'n_samples': n_samples,
    }
    
    # Subsample if dataset is too large for expensive calculations
    if n_samples > max_samples:
        np.random.seed(42)  # For reproducibility
        indices = np.random.choice(n_samples, max_samples, replace=False)
        x_sub = x[indices]
        y_sub = y[indices]
        results['subsampled'] = True
        results['subsample_size'] = max_samples
    else:
        x_sub = x
        y_sub = y
        results['subsampled'] = False
    
    # Fast correlations (always compute on full data)
    from scipy.stats import pearsonr, spearmanr, kendalltau
    results['pearson_r'], results['pearson_p'] = pearsonr(x, y)
    results['spearman_r'], results['spearman_p'] = spearmanr(x, y)
    results['kendall_tau'], results['kendall_p'] = kendalltau(x, y)
    
    # Linear regression (fast)
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import r2_score
    X = x.reshape(-1, 1)
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)
    results['r2'] = r2_score(y, y_pred)
    results['slope'] = model.coef_[0]
    results['intercept'] = model.intercept_
    
    # Distance correlation (expensive - use subsampled data)
    try:
        import dcor
        # Use AVL method for better performance
        results['distance_corr'] = dcor.distance_correlation(x_sub, y_sub, method='AVL')
    except ImportError:
        results['distance_corr'] = np.nan
    except Exception:
        results['distance_corr'] = np.nan
    
    # MIC (expensive - use subsampled data)
    try:
        from minepy import MINE
        if len(x_sub) >= 50:
            mine = MINE(alpha=0.6, c=15)
            mine.compute_score(x_sub, y_sub)
            results['mic'] = mine.mic()
        else:
            results['mic'] = np.nan
    except ImportError:
        results['mic'] = np.nan
    except Exception:
        results['mic'] = np.nan
    
    return results


def calculate_diversity_metrics(abundances: np.ndarray) -> Dict[str, float]:
    """
    Calculate multiple diversity metrics from abundance data.
    
    Args:
        abundances: Array of abundances (counts or proportions)
        
    Returns:
        Dictionary of diversity metrics
    """
    # Filter out zeros and convert to numpy
    abundances = np.array(abundances)
    abundances = abundances[abundances > 0]
    
    if len(abundances) == 0:
        return {metric: np.nan for metric in DIVERSITY_METRICS}
    
    # Calculate proportional abundances
    total = abundances.sum()
    if total == 0:
        return {metric: np.nan for metric in DIVERSITY_METRICS}
    
    p = abundances / total
    
    # Observed Richness (S) - number of species/KOs
    S = len(abundances)
    
    # Shannon Index (H') = -Σ(p_i * ln(p_i))
    # Only use non-zero proportions to avoid log(0)
    shannon = -np.sum(p * np.log(p))
    
    # Simpson's Index (D) = Σ(p_i²)
    simpson_d = np.sum(p ** 2)
    
    # Gini-Simpson Index = 1 - D
    gini_simpson = 1 - simpson_d
    
    # Hill Number of Order 2 = 1/D (effective number of species)
    # Also called Simpson's reciprocal or inverse Simpson
    hill_2 = 1 / simpson_d if simpson_d > 0 else np.nan
    
    # Berger-Parker Index = n_max / N (dominance by most abundant)
    berger_parker = np.max(p)
    
    # Pielou's Evenness (J') = H' / ln(S)
    pielou = shannon / np.log(S) if S > 1 else np.nan
    
    return {
        'observed_richness': S,
        'shannon_index': shannon,
        'simpson_index': simpson_d,
        'gini_simpson': gini_simpson,
        'hill_2': hill_2,
        'berger_parker': berger_parker,
        'pielou_evenness': pielou
    }


def get_wgs_samples(config: Config) -> pd.DataFrame:
    """
    Query metadata database to get WGS metagenomic sample accessions.
    
    Returns:
        DataFrame with columns: acc, mbases
    """
    log_step("STEP 1: Querying WGS metagenomic samples")
    step_start = time.time()

    query = f"""
    SELECT 
        acc,
        mbases
    FROM metadata_geo_joined 
    WHERE assay_type = 'WGS' 
        AND libraryselection = 'RANDOM' 
        AND mbases > {config.min_mbases}
    ORDER BY acc
    """
    
    logger.info(f"Connecting to {METADATA_DB}...")
    conn = duckdb.connect(METADATA_DB, read_only=True, config={'threads': 1})
    
    try:
        logger.info("Executing query...")
        df = conn.execute(query).fetchdf()
        logger.info(f"Found {len(df):,} WGS metagenomic samples")
        
        if config.n_samples and len(df) > config.n_samples:
            logger.info(f"Randomly sampling {config.n_samples:,} samples (seed={config.random_seed})")
            df = df.sample(n=config.n_samples, random_state=config.random_seed)
            df = df.sort_values('acc').reset_index(drop=True)
        
        logger.info(f"Sample statistics:")
        logger.info(f"  Mbases range: {df['mbases'].min():.1f} - {df['mbases'].max():.1f}")
        logger.info(f"  Mbases mean: {df['mbases'].mean():.1f}")
        logger.info(f"  Mbases median: {df['mbases'].median():.1f}")
        
        # Save sample list
        sample_file = config.output_dir / "data" / "selected_samples.csv"
        df.to_csv(sample_file, index=False)
        logger.info(f"Saved sample list to: {sample_file}")
        
        elapsed = time.time() - step_start
        log_step(f"STEP 1: Querying WGS samples (took {elapsed:.1f}s)", start=False)
        
        return df
        
    finally:
        conn.close()


def process_sample_batch(sample_ids: List[str]) -> List[Dict]:
    """
    Process a batch of samples to extract hash counts and functional diversity metrics.
    
    This function is designed to be run in parallel.
    
    Returns:
        List of dicts with sample_id, num_hashes, and all diversity metrics
    """
    conn = duckdb.connect(YACHT_DB, read_only=True, config={'threads': 1})
    results = []
    
    try:
        for sample_id in sample_ids:
            try:
                # Query hash count from functional_profile_data.gather_data
                hash_query = f"""
                SELECT query_n_hashes 
                FROM functional_profile_data.gather_data 
                WHERE sample_id = '{sample_id}' 
                LIMIT 1
                """
                
                hash_result = conn.execute(hash_query).fetchdf()
                
                if hash_result.empty:
                    continue
                
                num_hashes = hash_result['query_n_hashes'].iloc[0]
                
                # Query KO abundances from functional_profile.profiles
                ko_query = f"""
                SELECT ko_id, abundance
                FROM functional_profile.profiles 
                WHERE sample_id = '{sample_id}'
                """
                
                ko_df = conn.execute(ko_query).fetchdf()
                
                if ko_df.empty:
                    continue
                
                # Calculate diversity metrics
                abundances = ko_df['abundance'].values
                diversity_metrics = calculate_diversity_metrics(abundances)
                
                # Build result dict
                result = {
                    'sample_id': sample_id,
                    'num_hashes': num_hashes,
                    'num_kos': len(ko_df),  # Raw count of KOs for reference
                }
                result.update(diversity_metrics)
                results.append(result)
                
            except Exception as e:
                # Skip samples that cause errors - logged by caller
                continue
                
    finally:
        conn.close()
    
    return results


def extract_hash_and_diversity_data(samples_df: pd.DataFrame, config: Config) -> pd.DataFrame:
    """
    Extract hash counts and functional diversity metrics for all samples using parallel processing.
    
    Args:
        samples_df: DataFrame with sample accessions and mbases
        config: Configuration object
        
    Returns:
        DataFrame with columns: sample_id, num_hashes, diversity metrics, mbases
    """
    log_step("STEP 2: Extracting hash counts and functional diversity metrics")
    step_start = time.time()
    
    logger.info(f"Using {config.n_jobs} parallel workers")
    
    # Split samples into batches for parallel processing
    sample_ids = samples_df['acc'].tolist()
    batch_size = max(1, len(sample_ids) // (config.n_jobs * 4))
    batches = [sample_ids[i:i+batch_size] for i in range(0, len(sample_ids), batch_size)]
    
    logger.info(f"Processing {len(sample_ids):,} samples in {len(batches)} batches")
    logger.info(f"Batch size: ~{batch_size} samples")
    
    # Process batches in parallel
    all_results = []
    extraction_start = time.time()
    
    logger.info("Starting parallel batch processing...")
    with ProcessPoolExecutor(max_workers=config.n_jobs) as executor:
        futures = {
            executor.submit(process_sample_batch, batch): i 
            for i, batch in enumerate(batches)
        }
        
        with tqdm(total=len(batches), desc="Processing batches") as pbar:
            for future in as_completed(futures):
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                except Exception as e:
                    logger.error(f"Error in batch: {e}")
                pbar.update(1)
    
    extraction_elapsed = time.time() - extraction_start
    logger.info(f"Batch processing complete in {extraction_elapsed:.1f}s ({len(sample_ids)/extraction_elapsed:.1f} samples/s)")
    
    # Convert to DataFrame
    logger.info("Converting results to DataFrame...")
    results_df = pd.DataFrame(all_results)
    
    if results_df.empty:
        logger.error("No data extracted! Check that samples exist in functional profile database.")
        sys.exit(1)
    
    logger.info(f"Successfully extracted data for {len(results_df):,} samples")
    logger.info(f"Samples with no data: {len(sample_ids) - len(results_df):,}")
    
    # Merge with mbases from original samples
    results_df = results_df.merge(
        samples_df[['acc', 'mbases']], 
        left_on='sample_id', 
        right_on='acc', 
        how='left'
    ).drop('acc', axis=1)
    
    # Calculate normalized metrics (per megabase)
    results_df['hashes_per_mb'] = results_df['num_hashes'] / results_df['mbases']
    
    # Normalize diversity metrics per megabase
    logger.info("Normalizing diversity metrics per megabase...")
    for metric in DIVERSITY_METRICS:
        if metric in results_df.columns:
            results_df[f'{metric}_per_mb'] = results_df[metric] / results_df['mbases']
    
    logger.info(f"Data summary:")
    logger.info(f"  Hashes range: {results_df['num_hashes'].min():,.0f} - {results_df['num_hashes'].max():,.0f}")
    logger.info(f"  Hashes per Mb range: {results_df['hashes_per_mb'].min():.2f} - {results_df['hashes_per_mb'].max():.2f}")
    logger.info(f"  Observed richness (KOs) range: {results_df['observed_richness'].min():.0f} - {results_df['observed_richness'].max():.0f}")
    logger.info(f"  Shannon index range: {results_df['shannon_index'].min():.2f} - {results_df['shannon_index'].max():.2f}")
    logger.info(f"  Hill 2 range: {results_df['hill_2'].min():.2f} - {results_df['hill_2'].max():.2f}")
    
    # Save raw data
    logger.info("Saving raw data to CSV...")
    data_file = config.output_dir / "data" / "functional_hash_diversity_data.csv"
    results_df.to_csv(data_file, index=False)
    logger.info(f"Saved raw data to: {data_file}")
    
    # Save parquet file with key columns for downstream analysis
    logger.info("Saving parquet file...")
    parquet_cols = [
        'sample_id', 'num_hashes', 'hashes_per_mb', 'mbases',
        'observed_richness', 'shannon_index', 'simpson_index', 'gini_simpson',
        'hill_2', 'berger_parker', 'pielou_evenness',
        'observed_richness_per_mb', 'shannon_index_per_mb', 'simpson_index_per_mb',
        'gini_simpson_per_mb', 'hill_2_per_mb', 'berger_parker_per_mb', 'pielou_evenness_per_mb'
    ]
    
    # Only include columns that exist
    parquet_cols = [c for c in parquet_cols if c in results_df.columns]
    
    # Create dataframe with renamed accession column for consistency
    parquet_df = results_df[parquet_cols].copy()
    parquet_df = parquet_df.rename(columns={'sample_id': 'accession', 'num_hashes': 'total_distinct_hashes'})
    
    # Add alpha_diversity and diversity_per_mb for compatibility with downstream scripts
    parquet_df['alpha_diversity'] = parquet_df['observed_richness']
    parquet_df['diversity_per_mb'] = parquet_df['observed_richness_per_mb']
    
    # Save as parquet
    parquet_file = config.output_dir / "data" / "functional_hash_diversity_data.parquet"
    parquet_df.to_parquet(parquet_file, index=False)
    logger.info(f"Saved parquet file to: {parquet_file}")
    
    step_elapsed = time.time() - step_start
    log_step(f"STEP 2: Data extraction (took {step_elapsed:.1f}s)", start=False)
    
    return results_df


def perform_correlation_analysis(df: pd.DataFrame, config: Config) -> Dict:
    """
    Perform correlation analysis between hashes and all diversity metrics.
    Uses parallel processing for expensive calculations (distance correlation, MIC).
    
    Args:
        df: DataFrame with hash and diversity data
        config: Configuration object
        
    Returns:
        Dictionary of analysis results for each metric
    """
    log_step("STEP 3: Correlation analysis (parallel)")
    step_start = time.time()
    
    # List of diversity metrics to correlate with hashes_per_mb
    metrics_to_analyze = [
        ('observed_richness_per_mb', 'Observed Richness per Mb'),
        ('shannon_index', 'Shannon Index (raw)'),
        ('simpson_index', 'Simpson Index (raw)'),
        ('gini_simpson', 'Gini-Simpson Index (raw)'),
        ('hill_2', 'Hill Number Order 2 (raw)'),
        ('berger_parker', 'Berger-Parker Index (raw)'),
        ('pielou_evenness', 'Pielou Evenness (raw)'),
    ]
    
    # Prepare tasks for parallel processing
    tasks = []
    for metric_col, metric_name in metrics_to_analyze:
        if metric_col not in df.columns:
            logger.warning(f"Skipping {metric_name}: column not found")
            continue
            
        df_clean = df.dropna(subset=['hashes_per_mb', metric_col])
        n_samples = len(df_clean)
        
        if n_samples < 10:
            logger.warning(f"Skipping {metric_name}: insufficient data (n={n_samples})")
            continue
        
        x = df_clean['hashes_per_mb'].values.copy()
        y = df_clean[metric_col].values.copy()
        
        tasks.append((metric_col, metric_name, x, y, config.max_samples_for_expensive_corr))
        logger.info(f"Queued {metric_name}: {n_samples:,} samples")
    
    if not tasks:
        logger.error("No metrics to analyze!")
        return {}
    
    logger.info(f"Processing {len(tasks)} metrics in parallel...")
    logger.info(f"Max samples for expensive correlations: {config.max_samples_for_expensive_corr:,}")
    
    # Process metrics in parallel
    all_results = {}
    parallel_start = time.time()
    
    # Use min of n_jobs and number of tasks
    n_workers = min(config.n_jobs, len(tasks))
    logger.info(f"Using {n_workers} parallel workers for correlation calculations")
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        futures = {executor.submit(compute_expensive_correlations, task): task[0] 
                   for task in tasks}
        
        for future in as_completed(futures):
            metric_col = futures[future]
            try:
                results = future.result()
                all_results[metric_col] = results
                
                # Log the results
                logger.info(f"  {results['metric_name']}:")
                logger.info(f"    n={results['n_samples']:,}, Pearson r={results['pearson_r']:.4f}, "
                           f"Spearman ρ={results['spearman_r']:.4f}, R²={results['r2']:.4f}")
                if not np.isnan(results['distance_corr']):
                    subsampled_str = f" (subsampled to {results.get('subsample_size', 'N/A')})" if results.get('subsampled', False) else ""
                    logger.info(f"    Distance corr={results['distance_corr']:.4f}{subsampled_str}")
                if not np.isnan(results['mic']):
                    logger.info(f"    MIC={results['mic']:.4f}")
                    
            except Exception as e:
                logger.error(f"Error computing correlations for {metric_col}: {e}")
    
    parallel_elapsed = time.time() - parallel_start
    logger.info(f"Parallel correlation calculations complete in {parallel_elapsed:.1f}s")
    
    step_elapsed = time.time() - step_start
    log_step(f"STEP 3: Correlation analysis (took {step_elapsed:.1f}s)", start=False)
    
    return all_results


def create_visualizations(df: pd.DataFrame, results: Dict, config: Config):
    """
    Create publication-quality visualizations for all diversity metrics.
    """
    log_step("STEP 4: Creating visualizations")
    step_start = time.time()
    
    for metric_col, metric_results in results.items():
        metric_name = metric_results['metric_name']
        logger.info(f"Creating plots for: {metric_name}")
        
        df_clean = df.dropna(subset=['hashes_per_mb', metric_col])
        
        if len(df_clean) < 10:
            continue
        
        # Main scatter plot with regression line
        fig, ax = plt.subplots(figsize=config.figure_size)
        
        ax.scatter(
            df_clean['hashes_per_mb'], 
            df_clean[metric_col],
            alpha=0.3,
            s=20,
            c='steelblue',
            edgecolors='none',
            rasterized=True
        )
        
        # Add regression line
        X = df_clean['hashes_per_mb'].values
        y_pred = metric_results['slope'] * X + metric_results['intercept']
        sort_idx = np.argsort(X)
        ax.plot(X[sort_idx], y_pred[sort_idx], 'r-', linewidth=2, label='Linear fit', zorder=10)
        
        # Labels and title
        ax.set_xlabel('Functional k-mer hash density per megabase\n(11-mer AA FracMinHash, scale=1000)', 
                      fontsize=14, fontweight='bold')
        ax.set_ylabel(metric_name, fontsize=14, fontweight='bold')
        ax.set_title(f'Correlation: Hash Density vs {metric_name}\nin WGS Metagenomic Samples',
                     fontsize=16, fontweight='bold', pad=20)
        
        # Stats box
        stats_text = f'n = {metric_results["n_samples"]:,}\n\n'
        stats_text += 'Linear:\n'
        stats_text += f'  Pearson r = {metric_results["pearson_r"]:.4f}\n'
        stats_text += f'  R² = {metric_results["r2"]:.4f}\n'
        stats_text += f'  p = {metric_results["pearson_p"]:.2e}\n\n'
        stats_text += 'Monotonic:\n'
        stats_text += f'  Spearman ρ = {metric_results["spearman_r"]:.4f}\n'
        stats_text += f'  Kendall τ = {metric_results["kendall_tau"]:.4f}\n\n'
        stats_text += 'Non-linear:\n'
        if not np.isnan(metric_results.get('distance_corr', np.nan)):
            stats_text += f'  Distance corr = {metric_results["distance_corr"]:.4f}\n'
        if not np.isnan(metric_results.get('mic', np.nan)):
            stats_text += f'  MIC = {metric_results["mic"]:.4f}'
        
        ax.text(
            0.05, 0.95, stats_text,
            transform=ax.transAxes,
            fontsize=11,
            verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
            family='monospace'
        )
        
        ax.legend(loc='lower right', fontsize=12)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # Create safe filename
        safe_name = metric_col.replace('/', '_').replace(' ', '_')
        plot_file = config.output_dir / "plots" / f"hash_vs_{safe_name}_correlation.png"
        plt.savefig(plot_file, dpi=config.dpi, bbox_inches='tight')
        plt.close()
        logger.info(f"  Saved: {plot_file.name}")
    
    # Create summary multi-panel figure
    logger.info("Creating summary multi-panel figure...")
    create_summary_figure(df, results, config)
    
    # Create hexbin for main metric (observed richness)
    logger.info("Creating hexbin plot...")
    create_hexbin_plot(df, config)
    
    # Create distribution plots
    logger.info("Creating distribution plots...")
    create_distribution_plots(df, config)
    
    step_elapsed = time.time() - step_start
    log_step(f"STEP 4: Visualizations (took {step_elapsed:.1f}s)", start=False)


def create_summary_figure(df: pd.DataFrame, results: Dict, config: Config):
    """Create a multi-panel summary figure showing all metrics."""
    
    n_metrics = len(results)
    if n_metrics == 0:
        return
    
    n_cols = min(3, n_metrics)
    n_rows = int(np.ceil(n_metrics / n_cols))
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
    if n_metrics == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    for idx, (metric_col, metric_results) in enumerate(results.items()):
        ax = axes[idx]
        df_clean = df.dropna(subset=['hashes_per_mb', metric_col])
        
        if len(df_clean) < 10:
            ax.text(0.5, 0.5, 'Insufficient data', ha='center', va='center', transform=ax.transAxes)
            continue
        
        ax.scatter(
            df_clean['hashes_per_mb'],
            df_clean[metric_col],
            alpha=0.3, s=10, c='steelblue', edgecolors='none', rasterized=True
        )
        
        # Regression line
        X = df_clean['hashes_per_mb'].values
        y_pred = metric_results['slope'] * X + metric_results['intercept']
        sort_idx = np.argsort(X)
        ax.plot(X[sort_idx], y_pred[sort_idx], 'r-', linewidth=2, alpha=0.8)
        
        # Stats annotation
        stats_text = f"ρ={metric_results['spearman_r']:.3f}\nr={metric_results['pearson_r']:.3f}\nR²={metric_results['r2']:.3f}"
        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=9,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                family='monospace')
        
        ax.set_xlabel('Hashes per Mb', fontsize=10)
        ax.set_ylabel(metric_results['metric_name'].replace(' per Mb', '').replace(' (raw)', ''), fontsize=10)
        ax.set_title(metric_results['metric_name'].replace(' per Mb', '').replace(' (raw)', ''), 
                     fontsize=11, fontweight='bold')
        ax.grid(True, alpha=0.3)
    
    # Hide unused subplots
    for idx in range(len(results), len(axes)):
        axes[idx].set_visible(False)
    
    fig.suptitle('Functional Hash-Diversity Correlations: All Metrics', 
                 fontsize=14, fontweight='bold', y=0.995)
    
    plt.tight_layout()
    plt.savefig(config.output_dir / "plots" / "summary_all_metrics.png", 
                dpi=config.dpi, bbox_inches='tight')
    plt.close()
    logger.info("Saved summary figure: summary_all_metrics.png")


def create_hexbin_plot(df: pd.DataFrame, config: Config):
    """Create hexbin plot for observed richness."""
    df_clean = df.dropna(subset=['hashes_per_mb', 'observed_richness_per_mb'])
    
    if len(df_clean) < 10:
        return
    
    fig, ax = plt.subplots(figsize=config.figure_size)
    hexbin = ax.hexbin(
        df_clean['hashes_per_mb'],
        df_clean['observed_richness_per_mb'],
        gridsize=50,
        cmap='YlOrRd',
        mincnt=1,
        bins='log'
    )
    ax.set_xlabel('Functional k-mer hash density per megabase\n(11-mer AA FracMinHash, scale=1000)', 
                  fontsize=14, fontweight='bold')
    ax.set_ylabel('Functional Richness (KOs) per megabase', fontsize=14, fontweight='bold')
    ax.set_title('Functional Hash Density vs KO Richness (Hexbin Density)', 
                 fontsize=16, fontweight='bold')
    plt.colorbar(hexbin, ax=ax, label='log10(count)')
    plt.tight_layout()
    plt.savefig(config.output_dir / "plots" / "hash_diversity_hexbin.png", 
                dpi=config.dpi, bbox_inches='tight')
    plt.close()
    logger.info("Saved hexbin plot")


def create_distribution_plots(df: pd.DataFrame, config: Config):
    """Create distribution plots for key metrics."""
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    
    # Hashes per Mb
    if 'hashes_per_mb' in df.columns:
        ax = axes[0, 0]
        ax.hist(df['hashes_per_mb'].dropna(), bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel('Functional k-mer hash density per Mb\n(11-mer AA, scale=1000)')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Hash Density')
        ax.axvline(df['hashes_per_mb'].median(), color='red', linestyle='--', 
                   label=f'Median: {df["hashes_per_mb"].median():.2f}')
        ax.legend()
    
    # Observed Richness per Mb
    if 'observed_richness_per_mb' in df.columns:
        ax = axes[0, 1]
        ax.hist(df['observed_richness_per_mb'].dropna(), bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel('KO Richness per Mb')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Functional Richness per Mb')
        ax.axvline(df['observed_richness_per_mb'].median(), color='red', linestyle='--',
                   label=f'Median: {df["observed_richness_per_mb"].median():.4f}')
        ax.legend()
    
    # Shannon Index
    if 'shannon_index' in df.columns:
        ax = axes[0, 2]
        ax.hist(df['shannon_index'].dropna(), bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel('Shannon Index')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Shannon Index')
        ax.axvline(df['shannon_index'].median(), color='red', linestyle='--',
                   label=f'Median: {df["shannon_index"].median():.2f}')
        ax.legend()
    
    # Hill Number Order 2
    if 'hill_2' in df.columns:
        ax = axes[1, 0]
        ax.hist(df['hill_2'].dropna(), bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel('Hill Number (Order 2)')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Hill Number (Order 2)')
        ax.set_yscale('log')
    
    # Berger-Parker Index
    if 'berger_parker' in df.columns:
        ax = axes[1, 1]
        ax.hist(df['berger_parker'].dropna(), bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel('Berger-Parker Index (Dominance)')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Berger-Parker Index')
    
    # Pielou Evenness
    if 'pielou_evenness' in df.columns:
        ax = axes[1, 2]
        ax.hist(df['pielou_evenness'].dropna(), bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel("Pielou's Evenness")
        ax.set_ylabel('Frequency')
        ax.set_title("Distribution of Pielou's Evenness")
    
    plt.tight_layout()
    plt.savefig(config.output_dir / "plots" / "distributions.png", 
                dpi=config.dpi, bbox_inches='tight')
    plt.close()
    logger.info("Saved distribution plots")


def generate_report(df: pd.DataFrame, results: Dict, config: Config):
    """Generate a comprehensive text report of the analysis."""
    log_step("STEP 5: Generating report")
    step_start = time.time()
    
    report_file = config.output_dir / "reports" / "analysis_report.txt"
    
    with open(report_file, 'w') as f:
        f.write("="*70 + "\n")
        f.write("FUNCTIONAL HASH-DIVERSITY CORRELATION ANALYSIS REPORT\n")
        f.write("="*70 + "\n\n")
        
        f.write(f"Analysis Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Minimum Mbases: {config.min_mbases}\n")
        f.write(f"FracMinHash Parameters: k=11 (amino acid), scale=1000\n\n")
        
        f.write("-"*70 + "\n")
        f.write("DATA SUMMARY\n")
        f.write("-"*70 + "\n")
        f.write(f"Total samples analyzed: {len(df):,}\n\n")
        
        f.write("Raw Metrics:\n")
        f.write(f"  Distinct hashes:\n")
        f.write(f"    Range: {df['num_hashes'].min():,.0f} - {df['num_hashes'].max():,.0f}\n")
        f.write(f"    Mean: {df['num_hashes'].mean():,.0f}\n")
        f.write(f"    Median: {df['num_hashes'].median():,.0f}\n\n")
        
        f.write(f"  Observed KO Richness:\n")
        f.write(f"    Range: {df['observed_richness'].min():.0f} - {df['observed_richness'].max():.0f}\n")
        f.write(f"    Mean: {df['observed_richness'].mean():.2f}\n")
        f.write(f"    Median: {df['observed_richness'].median():.0f}\n\n")
        
        f.write(f"  Shannon Index:\n")
        f.write(f"    Range: {df['shannon_index'].min():.2f} - {df['shannon_index'].max():.2f}\n")
        f.write(f"    Mean: {df['shannon_index'].mean():.2f}\n")
        f.write(f"    Median: {df['shannon_index'].median():.2f}\n\n")
        
        f.write(f"  Hill Number (Order 2):\n")
        f.write(f"    Range: {df['hill_2'].min():.2f} - {df['hill_2'].max():.2f}\n")
        f.write(f"    Mean: {df['hill_2'].mean():.2f}\n")
        f.write(f"    Median: {df['hill_2'].median():.2f}\n\n")
        
        f.write(f"  Sequencing depth (Mbases):\n")
        f.write(f"    Range: {df['mbases'].min():.1f} - {df['mbases'].max():.1f} Mb\n")
        f.write(f"    Mean: {df['mbases'].mean():.1f} Mb\n")
        f.write(f"    Median: {df['mbases'].median():.1f} Mb\n\n")
        
        f.write("-"*70 + "\n")
        f.write("CORRELATION ANALYSIS RESULTS\n")
        f.write("-"*70 + "\n\n")
        
        for metric_col, metric_results in results.items():
            f.write(f"Metric: {metric_results['metric_name']}\n")
            f.write("-"*50 + "\n")
            f.write(f"  Samples: {metric_results['n_samples']:,}\n")
            f.write(f"  Linear:\n")
            f.write(f"    Pearson r: {metric_results['pearson_r']:.6f} (p={metric_results['pearson_p']:.2e})\n")
            f.write(f"    R²: {metric_results['r2']:.6f}\n")
            f.write(f"    Regression: y = {metric_results['slope']:.8f}x + {metric_results['intercept']:.8f}\n")
            f.write(f"  Monotonic:\n")
            f.write(f"    Spearman ρ: {metric_results['spearman_r']:.6f} (p={metric_results['spearman_p']:.2e})\n")
            f.write(f"    Kendall τ: {metric_results['kendall_tau']:.6f} (p={metric_results['kendall_p']:.2e})\n")
            if not np.isnan(metric_results.get('distance_corr', np.nan)):
                f.write(f"  Non-linear:\n")
                f.write(f"    Distance correlation: {metric_results['distance_corr']:.6f}\n")
            if not np.isnan(metric_results.get('mic', np.nan)):
                f.write(f"    MIC: {metric_results['mic']:.6f}\n")
            f.write("\n")
        
        f.write("-"*70 + "\n")
        f.write("INTERPRETATION\n")
        f.write("-"*70 + "\n\n")
        
        # Summarize overall findings
        if 'observed_richness_per_mb' in results:
            r = results['observed_richness_per_mb']
            f.write(f"Primary finding (KO Richness):\n")
            f.write(f"  The relationship between hash density and functional richness\n")
            f.write(f"  shows Spearman ρ = {r['spearman_r']:.4f}, Pearson r = {r['pearson_r']:.4f}\n\n")
        
        f.write("Diversity metric comparisons:\n")
        for metric_col, metric_results in results.items():
            direction = "positive" if metric_results['spearman_r'] > 0 else "negative"
            strength = "strong" if abs(metric_results['spearman_r']) > 0.7 else "moderate" if abs(metric_results['spearman_r']) > 0.4 else "weak"
            f.write(f"  {metric_results['metric_name']}: {strength} {direction} (ρ={metric_results['spearman_r']:.3f})\n")
        
        f.write("\n")
        f.write("-"*70 + "\n")
        f.write("OUTPUT FILES\n")
        f.write("-"*70 + "\n")
        f.write(f"Data:\n")
        f.write(f"  {config.output_dir / 'data' / 'functional_hash_diversity_data.csv'}\n")
        f.write(f"  {config.output_dir / 'data' / 'functional_hash_diversity_data.parquet'}\n\n")
        f.write(f"Plots:\n")
        f.write(f"  {config.output_dir / 'plots' / 'summary_all_metrics.png'}\n")
        f.write(f"  {config.output_dir / 'plots' / 'hash_diversity_hexbin.png'}\n")
        f.write(f"  {config.output_dir / 'plots' / 'distributions.png'}\n")
        f.write(f"  (Individual metric plots in plots/ directory)\n\n")
        f.write(f"Reports:\n")
        f.write(f"  {config.output_dir / 'reports' / 'analysis_report.txt'}\n")
        f.write(f"  {config.output_dir / 'reports' / 'statistics_summary.csv'}\n\n")
        
        f.write("="*70 + "\n")
        f.write("END OF REPORT\n")
        f.write("="*70 + "\n")
    
    logger.info(f"Saved report to: {report_file}")
    
    # Save statistics as CSV
    logger.info("Saving statistics summary...")
    stats_rows = []
    for metric_col, metric_results in results.items():
        stats_rows.append(metric_results)
    
    stats_df = pd.DataFrame(stats_rows)
    stats_file = config.output_dir / "reports" / "statistics_summary.csv"
    stats_df.to_csv(stats_file, index=False)
    logger.info(f"Saved statistics to: {stats_file}")
    
    step_elapsed = time.time() - step_start
    log_step(f"STEP 5: Report generation (took {step_elapsed:.1f}s)", start=False)


def main():
    """Main analysis pipeline."""
    parser = argparse.ArgumentParser(
        description="Test correlation between distinct hashes and functional diversity",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        required=True,
        help='Output directory for results'
    )
    parser.add_argument(
        '--n-samples', '-n',
        type=int,
        default=None,
        help='Number of samples to analyze (default: all available samples)'
    )
    parser.add_argument(
        '--n-jobs', '-j',
        type=int,
        default=64,
        help='Number of parallel workers'
    )
    parser.add_argument(
        '--min-mbases', '-m',
        type=float,
        default=100.0,
        help='Minimum megabases for sample inclusion'
    )
    parser.add_argument(
        '--random-seed',
        type=int,
        default=42,
        help='Random seed for reproducibility'
    )
    parser.add_argument(
        '--dpi',
        type=int,
        default=300,
        help='DPI for saved figures'
    )
    parser.add_argument('--metric-subsample', type=int, default=50000, help='Number of subsampled data sets for the '
                                                                            'expensive metrics.')
    
    args = parser.parse_args()
    
    # Create configuration
    config = Config(
        output_dir=args.output_dir,
        n_samples=args.n_samples,
        n_jobs=args.n_jobs,
        min_mbases=args.min_mbases,
        random_seed=args.random_seed,
        dpi=args.dpi,
        max_samples_for_expensive_corr=args.metric_subsample
    )
    
    logger.info("="*70)
    logger.info("FUNCTIONAL HASH-DIVERSITY CORRELATION ANALYSIS")
    logger.info("="*70)
    logger.info(f"Configuration:")
    logger.info(f"  Output directory: {config.output_dir}")
    logger.info(f"  Number of samples: {'all' if config.n_samples is None else f'{config.n_samples:,}'}")
    logger.info(f"  Parallel workers: {config.n_jobs}")
    logger.info(f"  Minimum mbases: {config.min_mbases}")
    logger.info(f"  Random seed: {config.random_seed}")
    logger.info(f"  Max samples for dcor/MIC: {config.max_samples_for_expensive_corr:,}")
    logger.info(f"  FracMinHash: k=11 (amino acid), scale=1000")
    logger.info("="*70)
    
    start_time = time.time()
    
    # Step 1: Get WGS samples
    samples_df = get_wgs_samples(config)
    
    # Step 2: Extract hash and diversity data
    data_df = extract_hash_and_diversity_data(samples_df, config)
    
    # Step 3: Perform correlation analysis
    results = perform_correlation_analysis(data_df, config)
    
    # Step 4: Create visualizations
    create_visualizations(data_df, results, config)
    
    # Step 5: Generate report
    generate_report(data_df, results, config)
    
    elapsed = time.time() - start_time
    
    logger.info("="*70)
    logger.info("ANALYSIS COMPLETE")
    logger.info("="*70)
    logger.info(f"Total time: {elapsed/60:.1f} minutes")
    
    if 'observed_richness_per_mb' in results:
        r = results['observed_richness_per_mb']
        logger.info(f"Key findings (KO Richness):")
        logger.info(f"  Spearman ρ = {r['spearman_r']:.4f} (p = {r['spearman_p']:.2e})")
        logger.info(f"  Pearson r = {r['pearson_r']:.4f} (p = {r['pearson_p']:.2e})")
        logger.info(f"  R² = {r['r2']:.4f}")
        logger.info(f"  Samples analyzed: {r['n_samples']:,}")
    
    logger.info(f"Outputs saved to: {config.output_dir}/")
    logger.info("="*70)


if __name__ == "__main__":
    main()
